\chapter{Analisi Statica}

\begin{epigraphs}
\qitem{"As soon as we started  
programming, we found to  
our surprise that it wasn’t as  
easy to get programs right as  
we had thought.  Debugging  
had to be discovered. I can 
remember the exact instant  
when I realized that a large  
part of my life from then on  
was going to be spent in finding 
mistakes in my own programs."}{---\textsc{ Maurice Wilkes, Inventore di EDSAC, 1949}}
\end{epigraphs}

Tutti i progetti software condividono una caratteristica fondamentale: possiedono un codice sorgente che ne definisce il funzionamento. Tale codice sorgente è costituito da una serie di istruzioni scritte in un linguaggio di programmazione che vengono interpretate da un compilatore e successivamente eseguite. Il codice sorgente di un software risiede tipicamente su uno o più files di testo.\\
Il codice sorgente non è esente da errori bensì ha l'intrinseca proprietà di possedere difetti. Sin dagli albori della programmazione software gli sviluppatori hanno avuto a che fare con tali difetti, individuando un rapporto di proporzionalità diretta tra il numero di questi ultimi ed il numero di righe di codice scritte per un determinato software. L'aumentare della complessità dei programmi e la necessità di affidabilità hanno reso il controllo dei difetti fondamentale nell'industria del software, tanto che è opportuno che prima di un rilascio determinati standard di qualità siano rispettati. \\
Al fine di ridurre il quantitativo di difetti nel software e di aderire agli standard i programmatori hanno pensato di sviluppare altro software in grado di analizzare il codice sorgente di un programma durante la fase di sviluppo.\\
Tale analisi è detta \emph{analisi statica} ed è una tecnica che consiste nell'ispezionare automaticamente il codice sorgente di un software senza però eseguirlo. Il grosso vantaggio di tale tecnica consiste nella sua applicazione alla radice del processo di sviluppo, in contrasto alle esistenti tecniche di testing che vedevano posticipata la correzione dei difetti alla fase di pre-rilascio. Anticipare l'identificazione del difetto software comporta minori costi di correzione; è proprio questo il motivo del successo dell'analisi statica.

\section{Il problema}
Alan Touring\cite{CITAZIONE} nel 1937 affermò che esistono due tipologie di problemi: problemi decidibili, per i quali esiste un algoritmo (quindi una procedura eseguibile con un numero finito di passi) in grado di risolverli e problemi non decidibili, per i quali non esiste alcun algoritmo in grado di dare una risposta in tempo finito su tutte le istanze del problema. Henry Gordon Rice estese tale teorema, asserendo che ogni domanda non banale riguardante il comportamento di un programma è indecidibile. Di conseguenza non esiste alcun metodo meccanico per determinare se un dato programma può o non può generare errori di esecuzione.\\
L'analisi statica è un problema non decidibile. Alcuni fattori non possono infatti essere valutati se non dinamicamente e dipendono strettamente dalle condizioni di input. Essendo non decidibile, l'analisi statica genererà sempre falsi positivi e negativi e non garantisce che il codice analizzato sia corretto. L'accuratezza consiste nel determinare un buon livello di approssimazione al fine di ridurre i falsi.

\section{Storia}
Il primo approccio all'analisi statica è storicamente da attribuire al tool di Unix \emph{grep} (General Regular Expression Print), che ricerca in uno o più file di testo le linee che corrispondono ad uno o più modelli specificati con espressioni regolari o stringhe letterali e produce un elenco di linee in cui è stata trovata una corrispondenza.
Grep non conosce nessun dettaglio dei files che sta esaminando, li interpreta semplicemente come files di testo, di conseguenza sebbene possa produrre risultati apprezzabili con determinati pattern di ricerca non si può considerare come un vero e proprio tool di analisi statica. \\
Per aumentare la fedeltà nei risultati è importante che vengano prese in considerazione le regole lessicali specifiche del linguaggio, al fine di poter distinguere tra i vari costrutti.\\  
La vera nascita delle tecniche di analisi statica viene solitamente attribuita al tool Lint, sviluppato da Stephen C. Johnson e rilasciato alla fine degli anni '70. Lint fu realizzato allo scopo di segnalare come sospetti alcuni costrutti nel sorgente in linguaggio C, come la mancanza di punti e virgola, parentesi, cast impliciti, ecc. 
Lint era integrato con il processo di compilazione, soluzione che sembrava essere la migliore per riportare segnalazioni relative al codice e che ne contribuì alla diffusione.\\
Purtroppo le limitate capacità di analisi, quali ad esempio l'obbligo di eseguire la scansione un file per volta, fecero si che Lint riportasse un elevata percentuale di rumore tra i risultati, ovvero valori corretti dal punto di vista dell'analisi ma irrilevanti per lo sviluppatore al fine di correggere difetti. Ciò si tradusse nella necessità di eseguire dei controlli manuali sui risultati di Lint, esattamente la situazione che Lint si era proposto di eliminare.
Per tale motivo Lint non fu mai adottato globalmente come tool per l'individuazione di difetti.\\
Nei primi anni 2000 una seconda generazione di tools emerse, che si è evoluta fino ad oggi. Gli sviluppatori intuirono che era necessario comprendere attraverso il software di analisi maggiori dettagli relativi al funzionamento del programma. Produssero tools in grado di  analizzare più files contemporaneamente e di identificare i percorsi di flusso dei dati, ma si scontrarono con la problematica che da sempre caratterizza l'analisi statica: il necessario compromesso da attuare tra performance ed accuratezza. L'efficacia delle tecniche di analisi statica è altamente condizionata dal fatto che devono essere gli sviluppatori ad utilizzarle, poichè prima si è in grado di identificare il difetto e minore costo ha la sua correzione.

\section{Applicazioni}
Sebbene le tecniche di analisi statica nacquero allo scopo di individuare difetti e di aderire a standard nella stesura del codice, molteplici successivi utilizzi vennero identificati ed implementati. \\
Attualmente l'analisi statica è utilizzata negli IDE\footnote{Integrated Development Environment} per evidenziare la sintassi e le parole chiave, restituendo al programmatore una migliore visualizzazione del sorgente del programma ed aiutandolo a rintracciare facilmente errori di battitura.\\
Sempre negli IDE viene utilizzata per segnalare eventuali errori nell'adesione alle regole definite da standard di scrittura del codice, come le spaziature, i rientri, la lunghezza delle righe ed il posizionamento delle parentesi. Sebbene questi standard possano risultare ai più superflui, quando il progetto in sviluppo viene realizzato da un numero elevato di sviluppatori è importante uniformare il codice al fine di ottenere una codebase leggibile.\\
I software di ottimizzazione del codice sfruttano l'analisi statica per determinare porzioni di codice che possono essere eseguite più velocemente se riorganizzate; tale analisi viene svolta valutando come le istruzioni riempirebbero la pipeline della CPU e riordinandole di conseguenza.\\
L'analisi statica è utilizzata anche a fini di generazione della documentazione; tali librerie leggono il contenuto delle annotations e dei commenti all'interno del codice sorgente per generare documenti che descrivono la struttura del programma. Tra questi si ricorda Doxygen\footnote{http://www.doxygen.org}, una delle librerie più utilizzate a questo scopo.\\
L'analisi statica è infine anche usata dai programmatori per capire il funzionamento, per calcolare le metriche e per rifattorizzare il codice sorgente.\\
In caso di software che possiede requisiti di sicurezza, ed ultimamente sempre più spesso visto il diffondersi di applicazioni critiche sotto questo punto di vista, l'analisi statica consente di individuare codice potenzialmente vulnerabile.

A seconda dell'utilizzo e dell'implementazione un'analisi statica varia totalmente per velocità di esecuzione, dalla rapidità di un'esecuzione real-time alla lentezza di un'esecuzione alla ricerca di problematiche di sicurezza. La complessità del task risulta essere solitamente direttamente proporzionale al tempo necessario per l'esecuzione e ciò influenza gli utilizzi di tale tipologia di analisi. Tuttavia il principale scopo dell'analisi statica è quello di aiutare lo sviluppatore nel comprendere il codice e nel trovare e risolvere problemi, siano essi di sicurezza o meno.

\section{Analisi statica vs. analisi dinamica vs. code review}
Le tecniche di analisi statica analizzano il sorgente di un programma in modo automatico senza eseguirlo. Prima della nascita di tali tecniche gli sviluppatori effettuavano un controllo manuale sul codice chiamato \emph{code review}.\\

La code review può essere eseguita da più sviluppatori (\emph{peer review}) oppure da un solo sviluppatore. E' una procedura complessa, che ha come requisito fondamentale la piena conoscenza delle decisioni architetturali prese durante la progettazione e la scrittura del codice oltre ad un'ottima padronanza del linguaggio in analisi. \\
Esistono due categorie di code review: \emph{formal code review} e \emph{lightweight code review}. La prima categoria richiede un dettagliato processo di analisi suddiviso in molteplici fasi. Tale metodologia comporta l'analisi di copie stampate del materiale ed è svolta da più partecipanti che contemporaneamente analizzano il codice.\\
La seconda categoria richiede meno formalismi rispetto alla precedente e viene svolta solitamente durante il normale processo di sviluppo. All'interno di essa si possono individuare le seguenti pratiche:
\begin{itemize}
\item Over-the-shoulder: uno sviluppatore osserva il codice che l'altro sta scrivendo per segnalare eventuali problemi
\item Email pass around: un SCM\footnote{Source Code Management System} invia tramite email il nuovo codice inserito nella codebase ad un soggetto che si occupa di effettuare la review.
\item Pair programming: Due sviluppatori scrivono codice contemporaneamente sulla stessa workstation. 
\item Tool-assisted code review: Sviluppatori e reviewers usano tools in grado di effettuare code review collaborativa.
\end{itemize}
La problematica di questa tipologia di analisi consiste nel tempo che richiede; i dati raccolti dai maggiori operatori del settore stimano che in media si possa effettuare code review su 150 linee di codice per ogni ora, fino a rimuovere l'85\% dei difetti presenti nel software.

L'analisi dinamica è una tecnica che consiste nell'osservare il comportamento del software durante la sua esecuzione. Al fine di rendere tale tecnica effettiva è necessario che il programma venga eseguito con diversi input. L'analisi dinamica è una tecnica precisa, che non comporta approssimazione poichè osserva l'esatto comportamento runtime dell'applicazione.
Lo svantaggio dell'analisi dinamica è la sua specificità: i risultati proposti riguardano solo ed esclusivamente quell'esecuzione, non c'è garanzia che la test suite utilizzata esegua effettivamente tutti i possibili data flow all'interno del software e che quindi esegua ogni porzione di codice. L'approccio definito dall'analisi dinamica è particolarmente adatto per il testing ed il debugging.

Analisi statica ed analisi dinamica sono due approcci complementari che possono essere applicati allo stesso problema, i risultati hanno però diverse proprietà e l'esecuzione di ognuno ha diversi costi. Per tale ragione esistono soluzioni in grado di combinare analisi statica ed analisi dinamica al fine di ridurre i difetti tipici delle due tecniche e fornire risultati più attendibili.\\
Solitamente la combinazione di queste due tipologie consiste in tool di analisi dinamica che ispezionano dato in ingresso ed in uscita, usando la conoscenza ottenuta dall'analisi statica per aumentare l'accuratezza.\\
Un esempio di questa tipologie di tool è Saner, sviluppato da Balzarotti et. al. \ref{}, il quale utilizza un tool di analisi statica per effettuare il riconoscimento di routine custom  di sanitizzazione del codice, ed un'analisi dinamica successiva per valutarne l'efficacia. 

\chapter{Applicazione dell'analisi statica alla sicurezza di applicazioni web}

\begin{epigraphs}
\qitem{"If you spend more on coffee than on IT security, you will be hacked. What's more, you deserve to be hacked."}{---\textsc{ Richard Clarke, White House Cybersecurity Advisor}}
\end{epigraphs}

Analizzando i reports di più attacchi eseguiti con successo alla sicurezza di applicazioni web si è notato che molti di questi non sono rivolti alla scoperta di nuove tipologie di vulnerabilità, bensì alla ricerca di vulnerabilità note.\\ Sebbene sia facile imputare la continua presenza di tali vulnerabilità alla negligenza dello sviluppatore, il vero problema è dato dal fatto che le tecniche per evitare queste problematiche non sono codificate nel processo di sviluppo software e non è pensabile fare affidamento sulla memoria dello sviluppatore per evitarle. E' questo il motivo per cui occorrono dei sistemi in grado di rilevare tali vulnerabilità direttamente nel processo di sviluppo, ed è in questo caso che l'analisi statica entra in gioco.

I primi approcci all'analisi statica orientata alla sicurezza si possono trovare nei tools RATS, ITS4 e Flawfinder, capaci di effettuare il parsing del codice sorgente al fine di trovare chiamate a funzioni pericolose. Dopo aver trasformato in tokens il codice sorgente (il primo step che anche il compilatore esegue), questi tools effettuavano una comparazione tra lo stream di tokens generato ed una libreria di costrutti vulnerabili. Sebbene questi approcci basati sull'analisi lessicale furono un passo avanti rispetto a grep, producevano un elevato numero di falsi positivi. Uno stream di tokens è certamente meglio di uno stream di caratteri, ma nemmeno questi tools avevano una minima conoscenza del funzionamento del programma. \\
Con il passare degli anni i tool per l'analisi statica orientata alla sicurezza sono cresciuti in complessità e sono diventati più sofisticati. La vera evoluzione si è avuta con la presa in considerazione del contesto delle chiamate e delle informazioni semantiche di un programma. Questo ha reso possibile l'individuazione delle condizioni entro cui una vulnerabilità si può manifestare, aumentando l'accuratezza e l'efficienza.\\
Costruendo un AST (abstract syntax tree) dal codice sorgente questi tool possono effettivamente considerare la semantica del programma in esame. Dall'AST è possibile eseguire diverse tipologie di analisi:
\begin{itemize}
\item Local analysis: esamina il programma una funzione per volta e non considera le relazioni tra le funzioni.
\item Module-level analysis: considera un modulo/classe per volta ma non considera le chiamate tra i moduli.
\item Global-level analysis: considera il programma per intero prendendo in considerazione tutte le chiamate tra funzioni.
\end{itemize}
A seconda della tipologia di analisi dipende la quantità di contesto che il tool deve considerare, più contesto significa meno falsi positivi ma anche più computazione.\\
Un buon tool per l'analisi statica orientata alla sicurezza deve essere facile da usare ed i risultati devono essere comprensibili agli sviluppatori che non conoscono approfonditamente le problematiche di sicurezza. Al fine che un tool di analisi statica venga utilizzato come parte attiva nello sviluppo è importante che il tempo di computazione non sia troppo elevato viceversa rischia di essere utilizzato saltuariamente, aumentando il costo per la risoluzione di eventuali problematiche.

\section{Tecniche di analisi}
In letteratura sono presenti diversi approcci per l'esecuzione di analisi statica finalizzata alla scoperta di vulnerabilità di sicurezza. Tutti sono però contraddistinti dalla seguenti tre fasi:
\begin{itemize}
\item Costruzione del modello
\item Analisi
\item Report dei risultati
\end{itemize}

\subsection{Costruzione del modello}
La fase di costruzione del modello riguarda l'astrazione che è necessario effettuare sul codice sorgente. Come esposto in precedenza esistono tool che lavorano direttamente sul codice, il più primitivo dei quali è grep, tuttavia lavorare direttamente su di esso è problematico poiché è necessario costruire espressioni regolari complesse e non si ha una visione d'insieme dell'intero programma e delle procedure.\\
Nell'analisi statica solitamente si preprocessa il codice per ottenere una rappresentazione ad un livello più basso, sfruttando le normali operazioni eseguite dal compilatore:
\begin{enumerate}
\item Line reconstruction: nei linguaggi che consentono spaziature arbitrarie tra gli identificativi è necessaria la presenza di una fase che ricostruisca le linee restituendo una forma canonica. In alcuni vecchi linguaggi questa fase eseguiva anche una normalizzazione nel caso venisse usato lo stropping\footnote{Tecnica che consiste nell'utilizzare una stringa sia come parola chiave che come identificatore.}.
\item Analisi lessicale: il codice viene pulito da elementi inutili come spaziature e commenti e trasformato in uno stream di tokens. Ogni token è una singola unità atomica del linguaggio, ad esempio una parola chiave, un identificativo oppure il nome di un simbolo. Tool come RATS, ITS4 e Flawfinder si limitavano ad utilizzare il risultato di questa fase come input per la propria analisi.
\item Analisi sintattica: lo stream di tokens viene trasformato in un \emph{parse tree}, ovvero una rappresentazione ad albero del codice sorgente. Viene così determinata la struttura grammaticale dello stream di tokens in input grazie all'uso di una data grammatica formale. L'elemento che esegue questa operazione è detto \emph{parser}.
\item AST: il parse tree creato nella fase precedente viene trasformato in un \emph{abstract syntax tree}. Un AST è un albero simile al parse tree ma pulito di tutti i tokens non utili per l'analisi semantica.
\item Analisi semantica: Ad ogni token viene attribuito un significato al fine di creare una tabella dei simboli. Questa fase esegue il controllo dei tipi di dato (type checking), l'associazione tra le referenze delle funzioni e le loro definizioni (object binding), il controllo che tutte le variabili siano inizializzate prima dell'uso (definite assignment).
\item Analisi del flusso di controllo: Tutti i possibili percorsi che possono essere eseguiti all'interno del codice vengono tradotti in una serie di \emph{Control flow graphs}. Il flusso di controllo tra le funzioni è raccolto nei \emph{call graphs}.
\item Analisi del flusso dei dati: L'analisi controlla come i dati si muovono all'interno del programma. Vengono a tale scopo usati i grafici realizzati nella fase precedente. Il compilatore usa tale analisi per allocare i registri, rimuovere codice non utilizzato ed ottimizzare l'uso di processore e memoria.
\end{enumerate}

Nonostante il compilatore esegua tutte queste fasi per l'interpretazione del codice sorgente, i tool di analisi statica generalmente si limitano ad alcune di queste operazioni prima di effettuare l'analisi. In base al dato che si analizza esistono innumerevoli approcci a tale fase, ognuno con le proprie caratteristiche e di conseguenza con risultati molto differenti tra loro.\\

\subsection{Analisi}
Esistono generalmente due approcci per realizzare l'analisi statica di un'applicazione: \emph{local analysis} o \emph{intraprocedural analysis} e \emph{global analysis} o \emph{interprocedural analysis}.

La local analysis si occupa di analizzare una funzione individualmente, tenendo traccia delle proprietà dei dati nella funzione e delle condizioni per le quali una funzione può essere chiamata in modo sicuro. Tracciare tali proprietà diventa però problematico in caso di loop e branches, siccome è necessario salvare lo stato di tutti i dati in ogni momento dell'esecuzione la quantità degli stessi diventa ingente rendendo l'approccio poco praticabile. Riducendo la precisione tuttavia si possono ottenere comunque risultati apprezzabili con quantità di dati inferiori, astraendo le proprietà del programma che non sono di interesse (abstract interpretation) come può essere l'ordine in cui le istruzioni vengono eseguite (flow-insensitive analysis).

A seconda dello stato globale del programma una funzione può riportare risultati differenti. La global analysis consiste nel comprendere il contesto entro il quale una funzione viene eseguita, per valutare le implicazioni conseguenti. A tale scopo è possibile eseguire una \emph{whole-program analysis} che consiste nel combinare in un'unica funzione tutto il codice che viene eseguito sostituendo le chiamate a funzioni con i relativi metodi. Tale tecnica non viene spesso utilizzata poiché porta alla generazione di un unico blocco di codice sorgente molto lungo che richiede molta memoria e molto tempo per essere analizzato.\\
Una tecnica alternativa è chiamata \emph{function summaries} e consiste nel trasformare una funzione in un insieme di precondizioni e postcondizioni, usando la conoscenza ottenuta tramite la local analysis (talvolta infatti le due tecniche vengono combinate per affinare i risultati). Quando viene eseguita l'analisi si tengono in considerazione solo tali elementi per determinare gli effetti che la funzione ha sull'intero programma. 

Dopo aver determinato un'approccio all'analisi statica occorre definire delle policy affinché sia possibile rintracciare all'interno del flusso di istruzioni le eventuali vulnerabilità. Ogni vulnerabilità viene quindi analizzata singolarmente e vengono individuati gruppi di regole che ne permettono l'individuazione.\\
Pixy, un tool di analisi statica orientato alla scoperta di vulnerabilità, richiede che le regole vengano specificate attraverso un file di testo, che definisce le funzioni di sanitizzazione ed i sinks. Altri tool però lavorano in modo differente, basandosi sulle annotations (Splint ad esempio), altri ancora accettano la definizione delle regole mediante l'uso di un linguaggio apposito (solitamente PQL).

\subsection{Report dei risultati}
Al termine della fase di analisi è necessario che i risultati vengano processati per essere mostrati all'utente. Questa fase è molto importante, il valore di un sistema di analisi statico è infatti direttamente collegato alla sua capacità di riportare risultati in modo comprensibile ed immediato per l'utente.\\
Tra i possibili risultati di un'analisi figurano: falsi positivi, falsi negativi, veri positivi e veri negativi. I falsi positivi si verificano quando una parte del codice è segnalata impropriamente come vulnerabile, i falsi negativi quando non viene rilevata la presenza di una vulnerabilità esistente. La seconda categoria di falsi è più problematica poiché fornisce un falso senso di sicurezza. I veri positivi indicano una vulnerabilità correttamente identificata, i veri negativi indicano l'assenza di una segnalazione per una porzione di codice sicura.

In vari tool i risultati vengono categorizzati in base alla criticità, determinata tenendo conto dell'accuratezza con cui tale vulnerabilità viene rilevata, in altri si tiene conto della profondità, ovvero del numero di chiamate e di loop/branches da analizzare.\\
Talvolta insieme alla segnalazione viene mostrato un help contestuale che motiva all'utente la presenza della stessa, al fine di renderne maggiormente comprensibile la natura.