\chapter{Applicazione dell'analisi statica alla sicurezza di applicazioni web}

\begin{epigraphs}
\qitem{"If you spend more on coffee than on IT security, you will be hacked. What's more, you deserve to be hacked."}{---\textsc{ Richard Clarke, White House Cybersecurity Advisor}}
\end{epigraphs}

Analizzando i reports di più attacchi eseguiti con successo alla sicurezza di applicazioni web si è notato che molti di questi non sono rivolti alla scoperta di nuove tipologie di vulnerabilità, bensì alla ricerca di vulnerabilità note.\\ Sebbene sia facile imputare la continua presenza di tali vulnerabilità alla negligenza dello sviluppatore, il vero problema è dato dal fatto che le tecniche per evitare queste problematiche non sono codificate nel processo di sviluppo software e non è pensabile fare affidamento sulla memoria dello sviluppatore per evitarle. E' questo il motivo per cui occorrono dei sistemi in grado di rilevare tali vulnerabilità direttamente nel processo di sviluppo, ed è in questo caso che l'analisi statica entra in gioco.

I primi approcci all'analisi statica orientata alla sicurezza si possono trovare nei tools RATS\cite{rats}, ITS4\cite{its4} e Flawfinder\cite{flawfinder}, capaci di effettuare il parsing del codice sorgente al fine di trovare chiamate a funzioni pericolose. Dopo aver trasformato in tokens il codice sorgente (il primo step che anche il compilatore esegue), questi tools effettuavano una comparazione tra lo stream di tokens generato ed una libreria di costrutti vulnerabili. Sebbene questi approcci basati sull'analisi lessicale furono un passo avanti rispetto a grep, producevano un elevato numero di falsi positivi. Uno stream di tokens è certamente meglio di uno stream di caratteri, ma nemmeno questi tools avevano una minima conoscenza del funzionamento del programma. \\
Con il passare degli anni i tool per l'analisi statica orientata alla sicurezza sono cresciuti in complessità e sono diventati più sofisticati. La vera evoluzione si è avuta con la presa in considerazione del contesto delle chiamate e delle informazioni semantiche di un programma. Questo ha reso possibile l'individuazione delle condizioni entro cui una vulnerabilità si può manifestare, aumentando l'accuratezza e l'efficienza.\\
Costruendo un AST\footnote{Abstract Syntax Tree} dal codice sorgente questi tool possono effettivamente considerare la semantica del programma in esame. Dall'AST è possibile eseguire diverse tipologie di analisi:
\begin{itemize}
\item Local analysis: esamina il programma una funzione per volta e non considera le relazioni tra le funzioni.
\item Module-level analysis: considera un modulo/classe per volta ma non considera le chiamate tra i moduli.
\item Global-level analysis: considera il programma per intero prendendo in considerazione tutte le chiamate tra funzioni.
\end{itemize}
A seconda della tipologia di analisi dipende la quantità di contesto che il tool deve considerare, più contesto significa meno falsi positivi ma anche più computazione.\\
Un buon tool per l'analisi statica orientata alla sicurezza deve essere facile da usare ed i risultati devono essere comprensibili agli sviluppatori che non conoscono approfonditamente le problematiche di sicurezza. Per far si che un tool di analisi statica venga utilizzato come parte attiva nello sviluppo è importante che il tempo di computazione non sia troppo elevato viceversa rischia di essere utilizzato saltuariamente, fallendo il l'obiettivo per il quale è stato ideato.

\section{Tecniche di analisi}
In letteratura sono presenti diversi approcci per l'esecuzione di analisi statica finalizzata alla scoperta di vulnerabilità di sicurezza. Tutti sono però contraddistinti dalla seguenti tre fasi:
\begin{itemize}
\item Costruzione del modello
\item Analisi
\item Report dei risultati
\end{itemize}

\subsection{Costruzione del modello}
La fase di costruzione del modello riguarda l'astrazione che è necessario effettuare sul codice sorgente. Come esposto in precedenza esistono tool che lavorano direttamente sul codice, il più primitivo dei quali è grep, tuttavia lavorare direttamente su di esso è problematico poiché è necessario costruire espressioni regolari complesse e non si ha una visione d'insieme dell'intero programma e delle procedure.\\
Nell'analisi statica solitamente si pre-processa il codice per ottenere una rappresentazione ad un livello più basso, sfruttando le normali operazioni eseguite dal compilatore:
\begin{enumerate}
\item Line reconstruction: nei linguaggi che consentono spaziature arbitrarie tra gli identificativi è necessaria la presenza di una fase che ricostruisca le linee restituendo una forma canonica. In alcuni vecchi linguaggi questa fase eseguiva anche una normalizzazione nel caso venisse usato lo stropping\footnote{Tecnica che consiste nell'utilizzare una stringa sia come parola chiave che come identificatore.}.
\item Analisi lessicale: il codice viene pulito da elementi inutili come spaziature e commenti e trasformato in uno stream di tokens. Ogni token è una singola unità atomica del linguaggio, ad esempio una parola chiave, un identificativo oppure il nome di un simbolo. Tool come RATS, ITS4 e Flawfinder si limitavano ad utilizzare il risultato di questa fase come input per la propria analisi.
\item Analisi sintattica: lo stream di tokens viene trasformato in un \emph{parse tree}, ovvero una rappresentazione ad albero del codice sorgente. Viene così determinata la struttura dello stream di tokens in input grazie all'uso di una data grammatica formale. L'elemento che esegue questa operazione è detto \emph{parser}.
\item AST: il parse tree creato nella fase precedente viene trasformato in un \emph{abstract syntax tree}. Un AST è un albero simile al parse tree ma pulito di tutti i tokens non utili per l'analisi semantica.
\item Analisi semantica: Ad ogni token viene attribuito un significato al fine di creare una tabella dei simboli. Questa fase esegue il controllo dei tipi di dato (type checking), l'associazione tra le referenze delle funzioni e le loro definizioni (object binding), il controllo che tutte le variabili siano state inizializzate prima dell'uso (definite assignment).
\item Analisi del flusso di controllo: Tutti i possibili percorsi che possono essere eseguiti all'interno del codice vengono tradotti in una serie di \emph{Control flow graphs}. Il flusso di controllo tra le funzioni è raccolto nei \emph{call graphs}.
\item Analisi del flusso dei dati: L'analisi controlla come i dati si muovono all'interno del programma. Vengono a tale scopo usati i grafici realizzati nella fase precedente. Il compilatore usa tale analisi per allocare i registri, rimuovere codice non utilizzato ed ottimizzare l'uso di processore e memoria.
\end{enumerate}

Nonostante il compilatore esegua tutte queste fasi per l'interpretazione del codice sorgente, i tool di analisi statica generalmente si limitano ad alcune di queste operazioni prima di effettuare l'analisi. In base al dato che si analizza esistono innumerevoli approcci a tale fase, ognuno con le proprie caratteristiche e di conseguenza con risultati molto differenti tra loro.

\subsection{Analisi}
Esistono generalmente due approcci per realizzare l'analisi statica di un'applicazione: \emph{local analysis} o \emph{intraprocedural analysis} e \emph{global analysis} o \emph{interprocedural analysis}.

La local analysis si occupa di analizzare una funzione individualmente, tenendo traccia delle proprietà dei dati nella funzione e delle condizioni per le quali una funzione può essere chiamata. Tracciare tali proprietà diventa però problematico in caso di loop e branches, siccome è necessario salvare lo stato di tutti i dati in ogni momento dell'esecuzione la quantità degli stessi diventa ingente rendendo l'approccio poco praticabile. Riducendo la precisione tuttavia si possono ottenere comunque risultati apprezzabili con quantità di dati inferiori, astraendo le proprietà del programma che non sono di interesse (abstract interpretation) come può essere l'ordine in cui le istruzioni vengono eseguite (flow-insensitive analysis).

A seconda dello stato globale del programma una funzione può riportare risultati differenti. La global analysis consiste nel comprendere il contesto entro il quale una funzione viene eseguita, per valutare le implicazioni conseguenti. A tale scopo è possibile eseguire una \emph{whole-program analysis} che consiste nel combinare in un'unica funzione tutto il codice che viene eseguito sostituendo le chiamate a funzioni con i relativi metodi. Tale tecnica non viene spesso utilizzata poiché porta alla generazione di un unico blocco di codice sorgente molto lungo che richiede molta memoria e molto tempo per essere analizzato.\\
Una tecnica alternativa è chiamata \emph{function summaries} e consiste nel trasformare una funzione in un insieme di precondizioni e postcondizioni, usando la conoscenza ottenuta tramite la local analysis (talvolta infatti le due tecniche vengono combinate per affinare i risultati). Quando viene eseguita l'analisi si tengono in considerazione solo tali elementi per determinare gli effetti che la funzione ha sull'intero programma. 

Dopo aver determinato un'approccio all'analisi statica occorre definire delle policy affinché sia possibile rintracciare all'interno del flusso di istruzioni le eventuali vulnerabilità. Ogni vulnerabilità viene quindi analizzata singolarmente e vengono individuati gruppi di regole che ne permettono l'individuazione.\\
Pixy\cite{pixy}, un tool di analisi statica orientato alla scoperta di vulnerabilità, richiede che le regole vengano specificate attraverso un file di testo, che definisce le funzioni di sanitizzazione ed i sinks. Altri tool però lavorano in modo differente, basandosi sulle annotations (Splint\cite{splint} ad esempio), altri ancora accettano la definizione delle regole mediante l'uso di un linguaggio apposito (solitamente PQL\cite{pql}).

\subsection{Report dei risultati}
Al termine della fase di analisi è necessario che i risultati vengano processati per essere mostrati all'utente. Questa fase è molto importante, il valore di un software di analisi statica è infatti direttamente collegato alla sua capacità di riportare risultati in modo comprensibile ed immediato per l'utente.\\
Tra i possibili risultati di un'analisi figurano: falsi positivi, falsi negativi, veri positivi e veri negativi. I falsi positivi si verificano quando una parte del codice è segnalata impropriamente come vulnerabile, i falsi negativi quando non viene rilevata la presenza di una vulnerabilità esistente. La seconda categoria di falsi è maggiormente problematica poiché fornisce un falso senso di sicurezza. I veri positivi indicano una vulnerabilità correttamente identificata, i veri negativi indicano l'assenza di una segnalazione per una porzione di codice sicura.

In vari tool i risultati vengono categorizzati in base alla criticità, determinata tenendo conto dell'accuratezza con cui tale vulnerabilità viene rilevata, in altri si tiene conto della profondità, ovvero del numero di chiamate e di loop/branches da analizzare.\\
Talvolta insieme alla segnalazione viene mostrato un help contestuale che motiva all'utente la presenza della stessa, al fine di renderne maggiormente comprensibile la natura.